"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import yaml
import torch
import numpy as np
import pandas as pd
import json
from pathlib import Path

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.models import load_model
from utils.metrics import evaluate_model, calculate_confusion_matrix, calculate_class_wise_metrics
from utils.visualization import plot_confusion_matrix
from notebooks.data_pipeline import create_dataloaders


def evaluate_all_models(config_path="configs/config.yaml"):
    """ëª¨ë“  ëª¨ë¸ í‰ê°€"""
    # ì„¤ì • ë¡œë“œ
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ë°ì´í„° ë¡œë” ìƒì„± (test_loader ì‚¬ìš© - ì§„ì§œ í…ŒìŠ¤íŠ¸ì…‹!)
    # ì£¼ì˜: í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ì‹œë“œ(42)ë¡œ ë¶„í• í•˜ë¯€ë¡œ ê°™ì€ test setì´ ìƒì„±ë©ë‹ˆë‹¤
    _, _, test_loader, class_names = create_dataloaders(
        data_dir=config["data"]["test_dir"] if os.path.exists(config["data"]["test_dir"]) else config["data"]["train_dir"],
        batch_size=config["data"]["batch_size"],
        img_size=config["data"]["img_size"]
    )
    
    # ë°ì´í„° ë¡œë” None ì²´í¬
    if test_loader is None or class_names is None:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨. í”„ë¡œê·¸ëž¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print(f"\nðŸ” Final Evaluation on {len(test_loader.dataset)} Test Images (Unseen Data)")
    
    results = {}
    
    # CNN ëª¨ë¸ í‰ê°€
    cnn_model_path = os.path.join(config["training"]["save_dir"], 
                                  f"cnn_{config['models']['cnn']['name']}_best.pth")
    if os.path.exists(cnn_model_path):
        print("\n" + "="*50)
        print("Evaluating CNN Model")
        print("="*50)
        
        cnn_model = load_model(
            cnn_model_path,
            model_type="cnn",
            model_name=config["models"]["cnn"]["name"],
            num_classes=config["models"]["cnn"]["num_classes"],
            device=device
        )
        
        metrics, y_true, y_pred = evaluate_model(cnn_model, test_loader, device)
        cm = calculate_confusion_matrix(y_true, y_pred)
        class_metrics = calculate_class_wise_metrics(y_true, y_pred, class_names)
        
        results["cnn"] = {
            "metrics": metrics,
            "confusion_matrix": cm.tolist(),
            "class_wise_metrics": class_metrics
        }
        
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"F1-Score: {metrics['f1_score']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        
        # Confusion Matrix ì €ìž¥
        save_dir = config["evaluation"]["save_dir"]
        os.makedirs(save_dir, exist_ok=True)
        plot_confusion_matrix(y_true, y_pred, class_names, 
                            os.path.join(save_dir, "cnn_confusion_matrix.png"))
    else:
        print(f"CNN model not found: {cnn_model_path}")
    
    # ViT ëª¨ë¸ í‰ê°€
    vit_model_path = os.path.join(config["training"]["save_dir"],
                                  f"vit_{config['models']['vit']['name']}_best.pth")
    if os.path.exists(vit_model_path):
        print("\n" + "="*50)
        print("Evaluating ViT Model")
        print("="*50)
        
        vit_model = load_model(
            vit_model_path,
            model_type="vit",
            model_name=config["models"]["vit"]["name"],
            num_classes=config["models"]["vit"]["num_classes"],
            device=device
        )
        
        metrics, y_true, y_pred = evaluate_model(vit_model, test_loader, device)
        cm = calculate_confusion_matrix(y_true, y_pred)
        class_metrics = calculate_class_wise_metrics(y_true, y_pred, class_names)
        
        results["vit"] = {
            "metrics": metrics,
            "confusion_matrix": cm.tolist(),
            "class_wise_metrics": class_metrics
        }
        
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"F1-Score: {metrics['f1_score']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall: {metrics['recall']:.4f}")
        
        # Confusion Matrix ì €ìž¥
        save_dir = config["evaluation"]["save_dir"]
        os.makedirs(save_dir, exist_ok=True)
        plot_confusion_matrix(y_true, y_pred, class_names,
                            os.path.join(save_dir, "vit_confusion_matrix.png"))
    else:
        print(f"ViT model not found: {vit_model_path}")
    
    # ê²°ê³¼ ë¹„êµ ë° ì €ìž¥
    if len(results) > 0:
        print("\n" + "="*50)
        print("Model Comparison")
        print("="*50)
        
        comparison_data = []
        for model_name, result in results.items():
            comparison_data.append({
                "Model": model_name.upper(),
                "Accuracy": result["metrics"]["accuracy"],
                "F1-Score": result["metrics"]["f1_score"],
                "Precision": result["metrics"]["precision"],
                "Recall": result["metrics"]["recall"]
            })
        
        df = pd.DataFrame(comparison_data)
        print(df.to_string(index=False))
        
        # ê²°ê³¼ ì €ìž¥
        save_dir = config["evaluation"]["save_dir"]
        os.makedirs(save_dir, exist_ok=True)
        
        # JSON ì €ìž¥
        with open(os.path.join(save_dir, "evaluation_results.json"), "w") as f:
            json.dump(results, f, indent=2)
        
        # CSV ì €ìž¥
        df.to_csv(os.path.join(save_dir, "model_comparison.csv"), index=False)
        
        print(f"\nResults saved to {save_dir}")


if __name__ == "__main__":
    evaluate_all_models()
